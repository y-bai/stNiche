#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
@Author: Yong Bai, yong.bai@hotmail.com
@Time: 2023/6/30 11:46
@License: (C) Copyright 2013-2023. 
@File: _loss.py
@Desc:

"""

import torch
import torch.nn as nn
import torch.nn.functional as F


def vae_loss(x, x_rec, mu, log_var, max_iter, curr_iter, device,
             beta=0.001, gamma=0.0001, c_max=25.0, model_type='vanilla_vae'):
    """

    Loss function of VAE. There are two types of VAE
        vanilla VAE
        beta-VAE: ref: https://arxiv.org/pdf/1804.03599.pdf

    Parameters
    ----------
    x
        raw input 2d tensor with shape (N_sample, N_feature)
    x_rec
        reconstructed input 2d tensor by VAE, with shape (N_sample, N_feature)
    mu
        mean tensor generated by VAE,
    log_var
        log variance tensor generated by VAE
    max_iter
        maximum number of iteration during training (e.g, max_epochs), used for beta-VAE
    curr_iter
        current iteration index (e.g, current epoch), used for beta-VAE
    device
        device used for training VAE
    beta
        weight used for vanilla VAE KL loss, default value = 0.001.
    gamma
        weight used for beta VAE KL loss, default value = 0.0001
    c_max
        maximum value of controllable value for beta VAE, default value = 25.0
    model_type
        types of VAE model, which would be: `vanilla_vae` or `beta_vae`

    Returns
    -------
        dictionary of losses

    """
    recons_loss = F.mse_loss(x_rec, x)
    kl_vae_loss = torch.mean(0.5 * torch.sum(-1 - log_var + mu ** 2 + log_var.exp(), dim=1), dim=0)

    if model_type == 'vanilla_vae':
        # vanilla VAE
        loss = recons_loss + beta * kl_vae_loss
    if model_type == 'beta_vae':
        # beta VAE
        c_max = torch.tensor([c_max]).to(device)
        c = torch.clamp(c_max / max_iter * curr_iter, 0, c_max.data[0])
        loss = recons_loss + gamma * (kl_vae_loss - c).abs()

    return {'total_loss': loss, 'recons_loss': recons_loss, 'KL_emb_loss': kl_vae_loss}


def ae_loss(x, x_rec, model_type='vanilla_ae', w=None, h=None, lam=1e-4):
    """

    Loss function of AE. There are two types of AE
        vanilla AE
        contractive AE: ref:

    Parameters
    ----------
    x
        raw input 2d tensor with shape (N_sample, N_feature)
    x_rec
        reconstructed input 2d tensor by VAE, with shape (N_sample, N_feature)
    model_type
        types of VAE model, which would be: `vanilla_ae` or `contractive_ae`
    w
        weight matrix of z layer(hidden layer) of AE
    h
        tensor output from the z layer
    lam
        weight factor for contractive loss, default value = 1e-4

    Returns
    -------

    """
    recons = F.mse_loss(x_rec, x)
    if model_type == 'vanilla_ae':
        loss = recons
        return {'total_loss': loss, 'recons_loss': recons}

    if model_type == 'contractive_ae':
        recons, contra_loss = _cae_loss(w, x, x_rec, h, lam)
        return {'total_loss': contra_loss, 'recons_loss': recons}


def dec_loss(q, p):
    """

    loss function of deep embedding clustering

    Parameters
    ----------
    q: q distribution
    p: p distribution (ie, target distribution)

    Returns
    -------

    """
    return F.kl_div(q.log(), p, reduction='batchmean')


def _cae_loss(w, x, x_rec, h, lam=1e-4):
    """
    contractive auto encoder loss

    ref
        https://github.com/avijit9/Contractive_Autoencoder_in_Pytorch/blob/master/CAE_pytorch.py
        https://agustinus.kristia.de/techblog/2016/12/05/contractive-autoencoder/

    Parameters
    ----------
    w
        weight matrix of z layer(hidden layer) of AE
    x
        raw input 2d tensor with shape (N_sample, N_feature)
    x_rec
        reconstructed input 2d tensor by VAE, with shape (N_sample, N_feature)
    h
        tensor output from the z layer
    lam
        weight factor for contractive loss, default value = 1e-4

    Returns
    -------

    """
    mse = F.mse_loss(x_rec, x)
    dh = h * (1 - h)  # N * n_hidden
    w_sum = torch.sum(nn.Parameter(w) ** 2, dim=1)   # w: (N_hidden x N)
    w_sum = w_sum.unsqueeze(1)  # shape N_hidden x 1

    # N x N_hidden * N_hidden x 1 = N x 1
    contractive_loss = torch.sum(torch.mm(dh ** 2, w_sum), 0)

    return mse, mse + contractive_loss.mul_(lam)
